\lhead{\emph{Introduction}}

% ***************************************************************************
\chapter{Introduction}
% ***************************************************************************

% ***************************************************************************
\section{Objectives}
% ***************************************************************************

The aim of this work is, given 2 point clouds from different kinect sensors, to merge them into a new point cloud containing all the information needed to build the 3D map of the workspace. To achieve this goal we already have some well know techniques to calibrate the sensors. \\
Until now, this project was using a registration process involving marker recognition. However, it appears that this process has some disadvantages. First, it requires to have a special printed pattern that can be recognized by a program that use it to estimate camera positions. Another problem is that the process is needed to be run each time the sensors are moved. Intentional movements of the cameras (when changing the workspace setup) may not be frequent but unexpected small rotations of the kinects may occur, for example a camera rotation of 1$^\circ$ will produce almost 2cm of error on a point at a distance of 2m from the camera. Along time this will lead to big errors in scene reconstruction and the robots may not be provided a 3D map accurate enough to behave as expected. \\
The purpose of this work is then to find a calibration method that fit better our requirements. In this section I will detail some of the main requirements of this work that I need to focus on to make sure that the end result will be useful for the other subsystems.

\subsection{Spatial accuracy}

In the first place, we need the result of this reconstruction process to be accurate. This means that the distance between any point provided in the point cloud and its real position in the real scene is small. In this project, the output point cloud will be used to build a voxelmap. This representation is using small cubes with fixed size to represent the scene. In the ideal case, the output point cloud contains points which position has a smaller error than this voxel size. In this case the map would represent accurately the position of each significant obstacle in the workspace as if it were computed from only one camera with a large view range. The common voxel size we use in this project is 2cm but it may vary depending on the task. However, the whole robot planning process couldn't rely entirely on this voxelmap which will mainly be used to segment planes and object to make decisions and create a rough trajectory planning that can be refined with more accurate embedded short range sensors. \\

\subsection{Processing speed}

The current process using marker board is now needing few minutes to provide accurate results. We want the new algorithm to run, in the worst case within the same amount of time which will allow us to use it as a calibration program. However, if this program is running faster, it could be called during the reconstruction process to recalibrate more frequently to make sure that no unexpected movement of the cameras has occurred. If fast enough, it could even be run in real time to estimate camera position in each frame which would be useful for tracking an embedded camera, installed on a moving robot for example.

\subsection{Simplicity}

The main aspect of this project is to evaluate how much we can automatize the registration process so that it doesn't involve any complicated process such as printing a template and moving it into the workspace for few minutes. The goal being to have a fully-automated process that doesn't requires any user action.

% ***************************************************************************
\section{Challenges} \label{int:chall}
% ***************************************************************************

In this particular project, we have to deal with unusual difficulties. Indeed, unlike most papers focusing on point cloud registration theory, we work with real noisy data from depth sensors instead of clean point clouds extracted from 3D models. Some papers add noise to their point clouds or use real point clouds (often laser scans that are much more precise) to test noise robustness of their algorithm. However we have to deal with other limitation from the kinects, even after careful intrinsic calibration, it remains image deformation, specially in the corners of the image. Differences in IR light reflection depending on the materials also deteriorate the quality of the point cloud by deforming or loosing some parts of the cloud. In addition we are not able to get a large overlapping region between the 2 point clouds for two reasons. The first is that, we want to cover a large scene and placing the kinects too close would reduce significantly the range of our system. The second reason is that kinects are active sensors that interfere within each other. This sensor use a \acrshort{ir} projector to evaluate the travelling time of light rays, if another sensor is watching the same region, \acrshort{ir} projections will interfere which leads to a nosier point cloud. \\
All of these limitations can be solved separately, but dealing with all of them in the same registration process requires to adapt some of the existing algorithms found in the literature. \\

% ***************************************************************************
\section{Scope}
% ***************************************************************************

This work is applied in a really particular project but the resulting technique could be applied in other cases as the aim of the project is not to write a solution to this particular scenario but to explore the possibility to reconstruct a scene with similar difficulties with as less knowledge as possible. My main assumption is that the scene is an indoor environment (kinect sensors can't be use outdoor) which gives us some clues about the geometry of the scene. Indoor environments are most likely to contains several big planes (walls, table or any flat furniture...). These planes provides reliable knowledge on camera orientation and assuming to be indoor will mainly allow me to be confident that we are able to detect planes in the input point clouds. \\
Also, I assume in this work that both camera intrinsic calibration is already solved, which means that, apart from calibration error, both clouds have similar scales (distances are the same within both point clouds) and non deformed (angles are the same in both point clouds). \\

In this work, I will first present some of the most used techniques in 3D point cloud registration defining their field of application as well as their assets and drawbacks. I will then describe how we can overcome the previously mentioned difficulties by combining some of these state of the art techniques. The next chapter will detail about the actual implementation of this solution and the final one will discuss the results and compare with other algorithms.
